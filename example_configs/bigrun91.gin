dataset.name = "images_256"
options.random_labels = True
options.num_classes = 1000
train_imagenet_transform.crop_method = "resize_with_pad"
options.z_dim = 512
options.g_flood = 0.05
options.d_flood = 0.20
flood_loss.enabled = True
stop_loss.enabled = True
#stop_loss.g_stop_g_below = 0.05 # Probably not necessary when using g_stop_d_above
stop_loss.g_stop_d_above = 1.50
stop_loss.d_stop_d_below = 0.20
resnet_biggan.Generator.blocks_with_attention = "128"
resnet_biggan.Discriminator.blocks_with_attention = "128"

options.architecture = "resnet_biggan_arch"
ModularGAN.conditional = True
options.batch_size = 1024
options.gan_class = @ModularGAN
options.lamba = 1
options.training_steps = 250000
weights.initializer = "orthogonal"
spectral_norm.singular_value = "auto"
spectral_norm.use_resource = True
spectral_norm.save_in_checkpoint = False
spectral_norm.power_iteration_rounds = 2

# Generator
G.batch_norm_fn = @conditional_batch_norm
G.spectral_norm = True
ModularGAN.g_use_ema = True
resnet_biggan.Generator.hierarchical_z = False
resnet_biggan.Generator.stylegan_z = True
G_mapping.mapping_lrmul = 1.0 # default: 0.01
G_mapping.normalize_function = 'sum'
resnet_biggan.Generator.embed_y = True
resnet_biggan.Generator.embed_y_dim = 512
standardize_batch.decay = 0.9
standardize_batch.epsilon = 1e-5
#standardize_batch.use_moving_averages = False
standardize_batch.use_moving_averages = True
#standardize_batch.use_cross_replica_mean = True
### NOTE: EvoNorm-s1 experimental use!
#standardize_batch.use_evonorm = True
#BigGanResNetBlock.use_relu = False
#resnet_biggan.Generator.use_relu = False
resnet_biggan.Generator.use_noise = True

# Discriminator
options.disc_iters = 2
D.spectral_norm = True
resnet_biggan.Discriminator.project_y = True
resnet_biggan.Discriminator.use_noise = True

# Loss and optimizer
loss.fn = @hinge
penalty.fn = @no_penalty
ModularGAN.g_lr = 0.0000666
ModularGAN.d_lr = 0.0005
ModularGAN.g_optimizer_fn = @tf.train.AdamOptimizer
ModularGAN.d_optimizer_fn = @tf.train.AdamOptimizer
tf.train.AdamOptimizer.beta1 = 0.0
tf.train.AdamOptimizer.beta2 = 0.999

z.distribution_fn = @tf.random.normal
eval_z.distribution_fn = @tf.random.normal

run_config.iterations_per_loop = 1000
run_config.save_checkpoints_steps = 1000

#resnet_biggan.Generator.channel_multipliers = "16, 16, 8, 8, 4, 2, 1"
#resnet_biggan.Discriminator.channel_multipliers = "1, 2, 4, 8, 8, 16, 16"

## Multi-dataset run: mix in e621-s+e621-s-Portraits, Danbooru2019 SFW 512, Danbooru2019 Portraits, Danbooru2019 Figures, Danbooru2019 PALM
## Total _n_=4,513,983 (command: `gsutil cat 'gs://dota-euw4a/datasets/e621-s/e621-s-filenames.txt' 'gs://dota-euw4a/datasets/e621-portraits-s-512/e621-portraits-s-512-filenames.txt' 'gs://dota-euw4a/datasets/danbooru2019-s/danbooru2019-s-filenames.txt' 'gs://dota-euw4a/datasets/portraits/portraits-filenames.txt' 'gs://dota-euw4a/datasets/danbooru2019figures/danbooru2019figures-filenames.txt' 'gs://dota-euw4a/datasets/palm/palm-filenames.txt'`)
options.datasets = "gs://dota-euw4a/datasets/e621-s/e621-s-0*,gs://dota-euw4a/datasets/e621-portraits-s-512/e621-portraits-s-512-0*,gs://dota-euw4a/datasets/danbooru2019-s/danbooru2019-s-0*,gs://dota-euw4a/datasets/portraits/portraits-0*,gs://dota-euw4a/datasets/danbooru2019figures/danbooru2019figures-0*,gs://dota-euw4a/datasets/palm/palm-0*"

## Single-dataset run: Figures only
#options.datasets = "gs://dota-euw4a/datasets/danbooru2019figures/danbooru2019figures-0*"

## Expand model size due to using multiple large datasets:
resnet_biggan.Generator.ch = 128
resnet_biggan.Discriminator.ch = 128

# balancing:
ModularGAN.g_lr_mul = 1.0
ModularGAN.d_lr_mul = 1.0
options.description = "Stylegan-Z test run"

G_main.style_mixing_prob = 0

noise_block.stddev = 0.00

discriminator/noise_block.noise_multiplier = 0.01
generator/noise_block.noise_multiplier = 0.01
